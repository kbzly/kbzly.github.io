<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Arm Lab</title>
      <link href="/2025/01/08/Arm%20Lab/"/>
      <url>/2025/01/08/Arm%20Lab/</url>
      
        <content type="html"><![CDATA[<h1 id="Robotic-Vision-and-Manipulation-System-for-Autonomous-Object-Handling"><a href="#Robotic-Vision-and-Manipulation-System-for-Autonomous-Object-Handling" class="headerlink" title="Robotic Vision and Manipulation System for Autonomous Object Handling"></a>Robotic Vision and Manipulation System for Autonomous Object Handling</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Given the challenge of utilizing an RX200 Arm to manipulate various types of blocks around the workspace, we applied our understanding of frame transformations, forward&#x2F;inverse kinematics, OpenCV, and programming to effectively complete these tasks. This approach allowed us to manipulate the arm, utilize camera information, detect blocks, and apply control schemes.</p><p>We document our process, methodology, and results to demonstrate our ability to apply the aforementioned concepts learned in class. Our methodology involves:</p><ol><li><strong>Camera Calibration</strong>: Calibrating the camera and implementing intrinsic, extrinsic, and homography matrices to convert between pixel, camera, and world coordinates.</li><li><strong>Kinematics</strong>: Using a DH table for forward kinematics to find the pose of the end-effector and inverse kinematics to calculate motor angles for specified poses.</li><li><strong>Block Detection</strong>: Implementing a block detector using OpenCV to create and fine-tune a blob detector for determining each block’s shape and color.</li><li><strong>Task Integration</strong>: Combining these components into a control scheme to accomplish tasks during a competition.</li></ol><p>We successfully completed most tasks in the competition, though some tasks were not scored.</p><h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><ul><li><strong>Precise Manipulation</strong>: Forward and inverse kinematics for accurate arm control.</li><li><strong>Real-Time Vision</strong>: OpenCV-based block detection, including blob detection for shapes and colors.</li><li><strong>Coordinate Transformation</strong>: Conversion between pixel, camera, and world coordinates using homography and transformation matrices.</li><li><strong>Task Automation</strong>: Integration of control schemes for competition tasks.</li></ul><h2 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h2><ol><li><p><strong>Camera Calibration</strong>:</p><ul><li>Implement intrinsic, extrinsic, and homography matrices.</li><li>Map coordinates between pixel, camera, and world frames.</li></ul></li><li><p><strong>Kinematics</strong>:</p><ul><li>Derive forward kinematics using the DH table to compute the pose of the end-effector.</li><li>Implement inverse kinematics to calculate motor angles for reaching specific end-effector poses.</li></ul></li><li><p><strong>Block Detection</strong>:</p><ul><li>Develop a blob detector using OpenCV.</li><li>Fine-tune detection algorithms to identify block shapes and colors accurately.</li></ul></li><li><p><strong>Control Scheme</strong>:</p><ul><li>Combine calibrated camera data, kinematics, and block detection into an integrated system.</li><li>Automate tasks for competition scenarios.</li></ul></li></ol><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><ul><li>Successfully completed most tasks in the competition.</li><li>Demonstrated integration of kinematics, vision, and control for robotic manipulation.</li></ul><h2 id="Repository"><a href="#Repository" class="headerlink" title="Repository"></a>Repository</h2><p>Find the project repository here: <a href="https://github.com/kbzly/Lab-Course-Part-1.git">GitHub</a></p><h2 id="Acknowledgements"><a href="#Acknowledgements" class="headerlink" title="Acknowledgements"></a>Acknowledgements</h2><p>This project was completed as part of the ARM Lab course at the University of Michigan. Special thanks to our instructors and team members for their guidance and collaboration.</p>]]></content>
      
      
      <categories>
          
          <category> Projects </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Camera </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> Robot Kinematics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ANA</title>
      <link href="/2025/01/08/ANA/"/>
      <url>/2025/01/08/ANA/</url>
      
        <content type="html"><![CDATA[<h1 id="Reproduction-and-Evaluation-of-Anytime-Nonparametric-A-Algorithm"><a href="#Reproduction-and-Evaluation-of-Anytime-Nonparametric-A-Algorithm" class="headerlink" title="Reproduction and Evaluation of Anytime Nonparametric A* Algorithm"></a>Reproduction and Evaluation of Anytime Nonparametric A* Algorithm</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li><p><strong>Reproduction</strong>: Reproduced the Anytime Nonparametric A* (ANA*) algorithm.</p></li><li><p><strong>Application</strong>: Designed specific robotic application scenarios to test the algorithm.</p></li><li><p><strong>Evaluation</strong>: Conducted a performance comparison with other path planning algorithms.</p></li><li><p><strong>Repository</strong>: <a href="https://github.com/kbzly/Anytime-Nonparametric-A-Algorithm.git">GitHub</a></p></li></ul><h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><ul><li><strong>Efficient Pathfinding</strong>: Uses nonparametric techniques to optimize performance and computational cost.</li><li><strong>Anytime Behavior</strong>: Quickly finds a feasible solution and refines it over time.</li><li><strong>Adaptability</strong>: Performs well in diverse robotic application scenarios.</li><li><strong>Comparison-Ready</strong>: Includes benchmarks against traditional path planning algorithms.</li></ul><h2 id="Pseudocode"><a href="#Pseudocode" class="headerlink" title="Pseudocode"></a>Pseudocode</h2><p>Here is an overview of the pseudocode for the <strong>Anytime Nonparametric A</strong>* algorithm:</p><p><img src="/images/ANA_pseudocode.png" alt="Pseudocode"></p><p><em>Figure: Pseudocode for the Anytime Nonparametric A</em> Algorithm.*</p><h2 id="Customized-Environment"><a href="#Customized-Environment" class="headerlink" title="Customized Environment"></a>Customized Environment</h2><p>Below is a sample customized environment used to evaluate the algorithm:</p><p><img src="/images/customized_environment.png" alt="Customized Environment"></p><p><em>Figure: Customized environment created for testing Anytime Nonparametric A</em> Algorithm.*</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Here is a sample result generated by the <strong>Anytime Nonparametric A*</strong> algorithm:</p><p><img src="/images/Result1.png" alt="Result"></p><p><em>Figure: Path planning result visualized for the customized environment.</em></p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>The Anytime Nonparametric A* (ANA*) algorithm offers the following insights:</p><ul><li>ANA* can quickly find a suboptimal solution and efficiently refine it to the optimal solution.</li><li>Its primary strength lies in adaptively maintaining the greediest search for fast suboptimal solutions rather than relying on a smart heuristic.</li><li>It is better suited to small but complex environments where its advantages can be fully utilized.</li><li>Using a <strong>Weighted Euclidean heuristic</strong> instead of a standard Euclidean heuristic can result in smoother paths in 8-connected spaces, making it more realistic for practical applications.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Projects </category>
          
      </categories>
      
      
        <tags>
            
            <tag> robotics </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YAVST</title>
      <link href="/2025/01/08/YAVST/"/>
      <url>/2025/01/08/YAVST/</url>
      
        <content type="html"><![CDATA[<h1 id="YAVST-Yet-Another-Video-Summarization-Tool"><a href="#YAVST-Yet-Another-Video-Summarization-Tool" class="headerlink" title="YAVST: Yet Another Video Summarization Tool"></a>YAVST: Yet Another Video Summarization Tool</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><strong>YAVST</strong> is a tool designed to automatically summarize long videos into short, engaging clips, suitable for platforms like TikTok. The tool leverages advanced machine learning techniques to identify the most meaningful video segments, ensuring the summarized videos maintain the essence of the original content.</p><p>By combining <strong>2D CNNs</strong> for video frame feature extraction and a <strong>transformer architecture</strong> for understanding temporal information, YAVST achieves state-of-the-art performance on the SumMe dataset, with an <strong>F1 score of 0.783</strong>.</p><h2 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h2><p>Video summarization requires understanding the identities and importance of objects within frame, as well as the dynamics of these objects across a sequence of frames. Pre-trained 2D convolutional neural network, such as ResNet, was employed to extract the features in a video frame. Extracted features was passed to a transformer network to gain temporal information. To mitigate disjoint frame summarization video was generated, kernel temporal segmentation (KTS) was employed like regularization to leverage high frame score and successive meaningful segments.</p><p><img src="/images/YAVST_workflow.png" alt="YAVST Workflow"></p><h2 id="Dataset-Description"><a href="#Dataset-Description" class="headerlink" title="Dataset Description"></a>Dataset Description</h2><p>TVSum and SumMe were chosen as dataset. TVSum contains 50 videos spanning 10 categories with lengths ranging from 2 to 10 minutes.  TVSum provides a segment-level annotation on the importance of each segment, on a scale of 1 (least important) to 5 (most important), with 20 sets of annotations for each video by different annotators. We converted the segment-level score to a frame level score by assigning the segment score to its constituent frames. SumMe also provides segment-level annotations indicating the importance of each segment within the video which designed for video summarization research.</p><p><img src="/images/YAVST_dataset.png" alt="YAVST dataset"></p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>Initially, video frames are processed through a ResNet model, a SOTA 2D CNN that can identify features within each frame efficiently. These identified features are then passed through XClip, which utilizes a SOTA cross-frame attention mechanism to analyze temporal relations and gather temporal information about events occurring across the frames, to generate a new set of features that captures the type of actions taking place in each frame. Finally, the features are weighted by a linear layer and passed to a sigmoid function to calculate the importance score for each frame.</p><p><img src="/images/YAVST_Architecture.png" alt="YAVST Architecture"></p><h2 id="Video-Summary-Generation-KTS"><a href="#Video-Summary-Generation-KTS" class="headerlink" title="Video Summary Generation: KTS"></a>Video Summary Generation: KTS</h2><p>Kernel temporal segmentation (KTS) technique was employed to ensure the continuity of the generated video. KTS method computes a set of ”change points” that segments the input features based on the pair-wise feature similarity, so that groups of similar features, corresponding to visually contiguous segments in our case, are generated.  </p><p><img src="/images/YAVST_KTS.png" alt="YAVST KTS"></p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>The Kernel Temporal Segmentation was using to get the breaking point of the video and decision of the down-sampled frame. According to the frame rate of the video, the down-sampled frame was broadcast to all frame of the original video to make the new summarized video. The decision of the original frame of the video could be used to calculate the F1 score to evaluate the model performance both in precision and recall. Kernel Temporal Segmentation takes a long time to compute, but fortunately the F1 score is high, representing the content emphasized by the generated video and its similarity to the ratings given by human users in the ground true validation set.   </p><table><thead><tr><th>Model</th><th>F1 Score</th></tr></thead><tbody><tr><td>vsLSTM</td><td>0.376</td></tr><tr><td>dppLSTM</td><td>0.386</td></tr><tr><td>HSA-RNN</td><td>0.441</td></tr><tr><td>SUM-GDA</td><td>0.528</td></tr><tr><td>SMN</td><td>0.583</td></tr><tr><td>ResNet+KTS</td><td>0.783</td></tr><tr><td>Modified X-Clip+ResNet</td><td>0.609</td></tr></tbody></table><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p> YAVST capitalizes on the powerful image feature extraction capabilities of ResNet 101and the sophisticated temporal comprehension of the XClip transformer. Our investigation into the performance of this combination shows its potential as a robust tool for video analysis task. Despite having a slightly lower overall F1 score compared to the original simpler XClip + ResNet 101 model. The modified one’s exhibited its ability to generate discriminative importance scores between frames. To address the limitation of frame-level importance scoring, we investigated on KTS technique to seek for a better performance on video summarization. We implemented a Resnet model embedding with KTS technique. This combination allowed our model to achieve a noteworthily high F1 score of 0.783 benchmark dataset. However, since KST requires the calculation of visual similarity, the calculation method has not been optimized, resulting in a large amount of computation and time required to generate frames for the final solution of the optimization problem.</p>]]></content>
      
      
      <categories>
          
          <category> Projects </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MBot Autonomous Ground Robot</title>
      <link href="/2025/01/08/Mbot/"/>
      <url>/2025/01/08/Mbot/</url>
      
        <content type="html"><![CDATA[<h1 id="MBot-Autonomous-Ground-Robot"><a href="#MBot-Autonomous-Ground-Robot" class="headerlink" title="MBot Autonomous Ground Robot"></a>MBot Autonomous Ground Robot</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Our project demonstrates key capabilities required by modern autonomous systems using the <strong>MBot</strong>, a three-level differential drive robot equipped with:</p><ul><li>DC motors and encoders for precise motion control.</li><li>A forward-facing camera for block detection using April Tags.</li><li>A LIDAR scanner for navigation and mapping.</li><li>A fabricated forklift mechanism for object interaction.</li></ul><p>The system integrates motion control, SLAM, path planning, and object manipulation, enabling the MBot to complete tasks like efficient exploration and object handling.</p><hr><h2 id="Motion-Controller-Odometry"><a href="#Motion-Controller-Odometry" class="headerlink" title="Motion Controller &amp; Odometry"></a>Motion Controller &amp; Odometry</h2><h3 id="Calibration"><a href="#Calibration" class="headerlink" title="Calibration"></a>Calibration</h3><p>MBot uses the Classic Calibrate script to calibrate the Mbot automatically and ensure that the polarity of the robot’s encoders match the software implementation. The calibration data is stored in the onboard Pico’s memory.  </p><h3 id="Odometry"><a href="#Odometry" class="headerlink" title="Odometry"></a>Odometry</h3><p>When comparing our actual Mbot rotation angle ω to a predetermined desired angle, we found a large error. To compensate, an updated method to set our angular velocity was implemented, which directly uses the gyro data to calculate rotation angle. We used Mbot dynamic model to calculate left and right wheel speed to update our odometry.   </p><h3 id="PID-Controller-and-Filters"><a href="#PID-Controller-and-Filters" class="headerlink" title="PID Controller and Filters"></a>PID Controller and Filters</h3><p>It is possible to drive the Mbot without a PID motor speed controller. However, applying an unfiltered pwm signal results in very jerky movements and the Mbot falls over more often than not. Implementing a proportional, integral, derivative controller allows for smooth and accurate speed adjustments. Our PID controller uses our calibration values as well as the wheel encoders to provide feedback on the wheel’s speed. The motor speed controller is implemented in the MBot loop function when the drive mode is “MODE MBOT VEL.” The measured speed of the motors, read using the motor’s encoders, is passed through a low pass filter to reduce the noise from discretization. The difference between the desired and measured velocity is used as an input parameter for the rc filter march which outputs the right and left motor command. The velocity command is then converted to a pwm signal.</p><h3 id="Control-Algorithm"><a href="#Control-Algorithm" class="headerlink" title="Control Algorithm"></a>Control Algorithm</h3><ul><li><strong>Straight Maneuver Controller:</strong> Adjusts velocity to maintain heading toward the goal.</li><li><strong>Turn Maneuver Controller:</strong> Controls angular velocity for precise rotations.</li><li><strong>Smart Maneuver Controller:</strong> Combines straight movement and angular adjustments.</li></ul><hr><h2 id="Simultaneous-Localization-and-Mapping-SLAM"><a href="#Simultaneous-Localization-and-Mapping-SLAM" class="headerlink" title="Simultaneous Localization and Mapping (SLAM)"></a>Simultaneous Localization and Mapping (SLAM)</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>SLAM combines data from LIDAR, odometry, and a particle filter to map and localize MBot. This is essential in environments without GPS or pre-existing maps.</p><h3 id="Mapping"><a href="#Mapping" class="headerlink" title="Mapping"></a>Mapping</h3><p>A 2D occupancy grid updates based on LIDAR scans:</p><ul><li>Endpoints of rays represent obstacles.</li><li>Points between the start and endpoint are marked as free using Bresenham’s Algorithm.</li></ul><h3 id="Action-Model"><a href="#Action-Model" class="headerlink" title="Action Model"></a>Action Model</h3><p>The MBot predicts its next pose by modeling its movements as a combination of:</p><ol><li>Rotation.</li><li>Translation.</li><li>Another rotation.</li></ol><p>Gaussian noise is added to account for uncertainties, balancing accuracy and efficiency.</p><h3 id="Sensor-Model-and-Particle-Filter"><a href="#Sensor-Model-and-Particle-Filter" class="headerlink" title="Sensor Model and Particle Filter"></a>Sensor Model and Particle Filter</h3><p>The particle filter evaluates potential poses based on LIDAR data. The most likely pose is determined by weighted particles, ensuring accurate localization.</p><hr><h2 id="Path-Planning"><a href="#Path-Planning" class="headerlink" title="Path Planning"></a>Path Planning</h2><h3 id="A-Path-Planning"><a href="#A-Path-Planning" class="headerlink" title="A* Path Planning"></a>A* Path Planning</h3><p>The A* algorithm finds the shortest path between two points using:</p><ul><li><strong>g-cost:</strong> Distance from the start to the current node.</li><li><strong>h-cost:</strong> Estimated distance to the goal.</li><li><strong>f-cost:</strong> Sum of g-cost and h-cost.</li></ul><p>Nodes are evaluated in 8 directions (up, down, left, right, and diagonals) for optimal path selection.</p><h3 id="Map-Exploration"><a href="#Map-Exploration" class="headerlink" title="Map Exploration"></a>Map Exploration</h3><p>The MBot explores using frontiers, which are free, unexplored areas identified by LIDAR. It:</p><ol><li>Plans a path to the nearest frontier.</li><li>Explores until no reachable frontiers remain.</li><li>Returns to the starting position.</li></ol><hr><h2 id="Forklift-Operations"><a href="#Forklift-Operations" class="headerlink" title="Forklift Operations"></a>Forklift Operations</h2><h3 id="Design-and-Fabrication"><a href="#Design-and-Fabrication" class="headerlink" title="Design and Fabrication"></a>Design and Fabrication</h3><p>The forklift uses a <strong>Scotch yoke mechanism</strong> for vertical movement, meeting three constraints:</p><ol><li>Aligns with pallets for pickup and clears a height &gt;120 mm for drop-off.</li><li>Minimizes obstruction of the camera and LIDAR.</li><li>Operates efficiently with a limited motor range.</li></ol><h3 id="Interacting-with-Crates"><a href="#Interacting-with-Crates" class="headerlink" title="Interacting with Crates"></a>Interacting with Crates</h3><ul><li><strong>Pickup Protocol:</strong> Identifies and lifts blocks using April Tags to determine ID, location, and height.</li><li><strong>Drop-Off Protocol:</strong> Places blocks in specified locations, ensuring proper stacking.</li></ul><p>The forklift operates at four preset heights to handle crates efficiently, based on experimentally determined positions.</p>]]></content>
      
      
      <categories>
          
          <category> Projects </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Autonomous Robots </tag>
            
            <tag> Path Planning </tag>
            
            <tag> SLAM </tag>
            
            <tag> LIDAR </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
