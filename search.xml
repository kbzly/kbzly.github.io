<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>resume</title>
      <link href="/2025/01/11/resume/"/>
      <url>/2025/01/11/resume/</url>
      
        <content type="html"><![CDATA[<p><a href="/downloads/resume.pdf" download target="_blank">Download My Resume</a></p>]]></content>
      
      
      <categories>
          
          <category> resume </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>A Watermark for Large Language Models</title>
      <link href="/2025/01/10/A-Watermark-for-Large-Language-Models/"/>
      <url>/2025/01/10/A-Watermark-for-Large-Language-Models/</url>
      
        <content type="html"><![CDATA[<h1 id="Reproduction-and-Evaluation-of-A-Watermark-for-Large-Language-Models"><a href="#Reproduction-and-Evaluation-of-A-Watermark-for-Large-Language-Models" class="headerlink" title="Reproduction and Evaluation of A Watermark for Large Language Models"></a>Reproduction and Evaluation of A Watermark for Large Language Models</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>As the adoption of large language models (LLMs) continues to expand, concerns around the authenticity and attribution of AI-generated content have grown. One emerging solution to address these concerns is the concept of <strong>watermarking</strong>‚Äîembedding identifiable signals into the output of LLMs. The watermarking approach proposed by Kirchenbauer et al. provides a robust and practical method to embed identifiable patterns in LLM outputs. </p><p>For more details, refer to the original paper: <a href="https://arxiv.org/abs/2301.10226">‚ÄúA Watermark for Large Language Models‚Äù</a>.</p><hr><h2 id="Why-Watermarking"><a href="#Why-Watermarking" class="headerlink" title="Why Watermarking?"></a>Why Watermarking?</h2><p>Watermarking serves as a mechanism to:</p><ul><li><strong>Identify AI-generated content</strong>: Influencing future LLM model training due to synthetic data.</li><li><strong>Ensure Accountability</strong>: Generating spam and disinformation on social media.</li><li><strong>Prevent Academic Cheating</strong>: Cheating on academic writing and coding exams.</li></ul><p>With the increasing sophistication of AI, watermarking provides a much-needed layer of security and transparency.</p><hr><h2 id="How-Does-Watermarking-Work"><a href="#How-Does-Watermarking-Work" class="headerlink" title="How Does Watermarking Work?"></a>How Does Watermarking Work?</h2><p>A signal is embedded in sequences generated by the watermarked language mode. There is a slight bias on the semantic space that will not be detected without the hash. The human language distribution can be assumed to be even because it did not know the information about the hash. However, the language model generated sequences with watermark bias on the distribution of that particular hash. The next token will be classified as green list and red list based on the former token without access language mode.   </p><p><img src="/images/Watermark/Watermark_sample.png" alt="Watermark sample"></p><hr><h2 id="ü§ó-Hugging-Face"><a href="#ü§ó-Hugging-Face" class="headerlink" title="ü§ó Hugging Face"></a>ü§ó <a href="https://huggingface.co/">Hugging Face</a></h2><p>The Hugging Face community is a vibrant ecosystem offering <strong>open-source machine learning models, tools, libraries, and datasets</strong> that empower developers and researchers to innovate effortlessly. With its user-friendly framework, Hugging Face enables the easy deployment, fine-tuning, and modification of state-of-the-art models, making it a go-to platform for natural language processing, computer vision, and more.</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h3><p>The <strong>Soft Red List algorithm</strong> works by adding a constant bias &delta; to the logits of tokens in the green list. The green list is determined based on the hash key and the preceding token. To divide the entire vocabulary, a random number generator is used to shuffle the numbers <code>0</code> to <code>V-1</code>, where <code>V</code> represents the vocabulary size. The first &gamma; * V tokens are categorized into the green list, while the remaining tokens are assigned to the red list.</p><p><img src="/images/Watermark/Watermark_generation.png" alt="Watermark generation"></p><h3 id="Detection"><a href="#Detection" class="headerlink" title="Detection"></a>Detection</h3><h3 id="Distribution-of-Human-Language-Tokens"><a href="#Distribution-of-Human-Language-Tokens" class="headerlink" title="Distribution of Human Language Tokens"></a>Distribution of Human Language Tokens</h3><p>The distribution of human language tokens can be described as follows:</p><h4 id="Mean-¬µ"><a href="#Mean-¬µ" class="headerlink" title="Mean (¬µ)"></a>Mean (¬µ)</h4><p>The mean is defined as the ratio of the number of tokens in the green list to the total number of tokens in the sequence. This represents the expected proportion of tokens that fall within the green list.</p><h4 id="Variance-œÉ¬≤"><a href="#Variance-œÉ¬≤" class="headerlink" title="Variance (œÉ¬≤)"></a>Variance (œÉ¬≤)</h4><p>The variance is derived from the binomial distribution and can be computed as:</p><p><img src="/images%5CWatermark%5Cformula1.png" alt="formula1"></p><p>where:</p><ul><li>T: Total number of tokens in the sequence,</li><li>p: The probability of a token being in the green list. For the generator, p corresponds to &gamma;, the expected fraction of green tokens.</li></ul><p>This formulation applies to both human-generated sequences and sequences generated by the model.</p><h4 id="Z-Test"><a href="#Z-Test" class="headerlink" title="Z-Test"></a>Z-Test</h4><p>A Z-test can then be employed to quantify the difference between the given sequence and natural human language. </p><ul><li><strong>Threshold</strong>: (z &#x3D; 4)<ul><li>This threshold implies that if a given sequence is classified as generated by a language model, the probability of it being human-generated is approximately (0.00006).</li></ul></li></ul><h3 id="Algorithm-Extension"><a href="#Algorithm-Extension" class="headerlink" title="Algorithm Extension"></a>Algorithm Extension</h3><p>We find that the soft watermark with constant Œ¥ still has two main drawbacks. Different models can have different vocabulary sizes. For instance, the Chinese model Qwen2.5 has a vocabulary size around 150000 while and opt model has a vocabulary size around the typical value 50000. This increased size will dilute the impact of Œ¥. Second, the Œ¥ is fixed for low-entropy text and high-entropy text. However, we want to have smaller Œ¥ for low-entropy token and relatively larger Œ¥ for high-entropy token to achieve lower perplexity while maintaining the watermark. We introduce the adaptive Œ¥ which takes the logits as input.  </p><p><img src="/images%5CWatermark%5Cformula2.png" alt="formula2"></p><p>The fraction k is used to normalized the effect on different vocabulary sizes. The entropy H is used to adjust Œ¥ for different input entropy. We also have a scaling factor Œ± and a bias factor Œ≤. In theory, this should achieve a better consistence over different models, provide a lower perplexity and keep the watermark detectable with high accuracy.</p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><h3 id="English-Experiment"><a href="#English-Experiment" class="headerlink" title="English Experiment"></a>English Experiment</h3><p>In replicating their results and conducting a basic analysis,  <strong>c4&#x2F;realnewslike</strong> was used as the English dataset and <strong>facebook&#x2F;opt-1.3b</strong> as the watermark model. The plots above display ROC curves along with Area Under the Curve(AUC) and perplexity (ppl) values. For a fixed Œ≥, increasing Œ¥ yields higher AUC but worse ppl. There is no clear trend when varying Œ≥. Overall, <strong>multinomial sampling</strong> achieves higher AUC at the expense of greater perplexity.</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><img src="/images/Watermark/result_en_1.png" alt="result_en_1"></td><td><img src="/images/Watermark/result_en_2.png" alt="result_en_2"></td></tr></tbody></table><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><img src="/images/Watermark/result_en_3.png" alt="result_en_3"></td><td><img src="/images/Watermark/result_en_4.png" alt="result_en_4"></td></tr></tbody></table><h3 id="Multilingual-Experiment"><a href="#Multilingual-Experiment" class="headerlink" title="Multilingual Experiment"></a>Multilingual Experiment</h3><p>Watermarking algorithm is also be extend to multilingual settings to see if it would still work well. Spanish, Chinese and Github Code dataset were tested and the Chinese dataset had a noticeably lower z-score for lower Œ¥ values. The reason was believed that this is due to the Chinese language having a significantly larger number of characters and vocab size than English. This leads to a dilution of the effect of Œ¥ as the distribution difference between the green list and red list will be decreased. </p><p><img src="/images/Watermark/spanish_dataset.png" alt="spanish_dataset"></p><p><img src="/images/Watermark/code_dataset.png" alt="code_dataset"></p><p><img src="/images/Watermark/chinese_dataset.png" alt="hinese_dataset"></p><p>The results report in the paper were also replicated, showing that Œ≥ &#x3D; 0.1 consistently lies on the Pareto front for balancing detection robustness and embedding overhead, regardless of Œ¥. As expected, reducing Œ≥ increases the z-score because it limits the available tokens in the green list. Conversely, increasing Œ¥ also raises the z-score by inflating the logits of green list tokens, making their generation more readily detectable.</p><p><img src="/images/Watermark/result_gamma.png" alt="result_gamma"></p><p><img src="/images/Watermark/result_delta.png" alt="result_delta"></p><h3 id="Adaptive-Œ¥-Experiment"><a href="#Adaptive-Œ¥-Experiment" class="headerlink" title="Adaptive Œ¥ Experiment"></a>Adaptive Œ¥ Experiment</h3><p>An experiment was conducted using Œ± &#x3D; 4.0 and Œ≤ &#x3D; 4.0 to approximate the performance of a fixed Œ¥ &#x3D; 2.0, while all other parameters were kept unchanged. Under the fixed Œ¥, a ppl of 7.5791 and an AUC of 0.9970 were observed. In contrast, with the adaptive Œ¥, a ppl of 7.4479 and an AUC of 0.9990 were recorded. Unlike previous findings, in which perplexity and AUC were shown to trade off, improvements in both were observed with the adaptive Œ¥, although the extent of improvement was minimal. If the dataset contains numerous low-entropy sequences, the improvement may become more significant.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h2><p> The watermarking algorithm from Kirchenbauer et al. was replicated and extended, embedding human-invisible but machine-detectable patterns in LLM-generated text. Robust performance was observed across English, Spanish, Chinese, and programming code, preserving detection accuracy with minimal quality loss. We confirmed that Œ≥ &#x3D; 0.1 remains on the Pareto front, balancing detection robustness and overhead. An adaptive Œ¥ mechanism was introduced to improve both perplexity and detection by adjusting bias based on token entropy and vocabulary size. These findings showcase watermarking‚Äôs flexibility across various contexts, laying a foundation for future refinements in detecting and mitigating improper use of LLM-generated content.</p>]]></content>
      
      
      <categories>
          
          <category> Projects </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Large Language Models </tag>
            
            <tag> AI Security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Structure Project</title>
      <link href="/2025/01/09/Data%20Structure%20Project/"/>
      <url>/2025/01/09/Data%20Structure%20Project/</url>
      
        <content type="html"><![CDATA[<p>This blog will list the projects I have completed in EECS281.</p><hr><h2 id="Project-1-Back-to-the-Ship"><a href="#Project-1-Back-to-the-Ship" class="headerlink" title="Project 1: Back to the Ship!"></a>Project 1: Back to the Ship!</h2><p>You have just broken out of the detention level of a large and strangely moon-like space station. You need to find your way back to your old spacecraft of questionable space-worthiness to escape. Your adorable robot friend has hacked into the elevator system of the space station to assist you in your escape.</p><p><a href="https://github.com/kbzly/Data-Structure-Project-1.git">Project GitHub Repository</a></p><h2 id="Project-2a-Stock-Market-Simulation"><a href="#Project-2a-Stock-Market-Simulation" class="headerlink" title="Project 2a: Stock Market Simulation"></a>Project 2a: Stock Market Simulation</h2><p>The <strong>Market Program</strong> is designed to simulate a stock trading system that processes and matches buy and sell orders for stocks. The program operates using priority queues to optimize order matching and supports various command-line options for detailed output.</p><p><a href="https://github.com/kbzly/Data-Structure-Project-2a.git">Project GitHub Repository</a></p><h2 id="Project-2b-Priority-Queues"><a href="#Project-2b-Priority-Queues" class="headerlink" title="Project 2b: Priority Queues"></a>Project 2b: Priority Queues</h2><p>The project involves implementing and testing three priority queues‚Äî<strong>Sorted Array Priority Queue</strong>, <strong>Binary Heap Priority Queue</strong>, and <strong>Pairing Heap Priority Queue</strong>‚Äîusing the provided <code>Eecs281PQ.hpp</code> interface and the <code>project2b.cpp</code> test module. Each priority queue is templated, defaults to max-priority behavior, and supports custom comparators like <code>std::greater&lt;&gt;</code> for min-priority behavior. The <strong>SortedPQ</strong> maintains a sorted array, ensuring O(n) insertion and O(1) access; the <strong>BinaryPQ</strong> uses a binary heap for O(log n) insertion and removal; and the <strong>PairingPQ</strong> uses a tree structure for amortized O(1) insertion and efficient melds. Validation involves comparing implementations against the provided inefficient but correct <strong>UnorderedPQ</strong>.</p><p><a href="https://github.com/kbzly/Data-Structure-Project-2b.git">Project GitHub Repository</a></p><h2 id="Project-3-281Bank"><a href="#Project-3-281Bank" class="headerlink" title="Project 3: 281Bank"></a>Project 3: 281Bank</h2><p>This project involves designing and implementing an <strong>online banking infrastructure</strong> that processes transactions efficiently while maintaining detailed records for audits, queries, and fraud detection. The system will simulate a <strong>real-time gross settlement (RTGS)</strong> system, where credits and debits are applied immediately and in the order they are executed. The primary focus is on ensuring accuracy, responsiveness, and scalability for handling transactions over time.</p><p><a href="https://github.com/kbzly/Data-Structure-Project-3.git">Project GitHub Repository</a></p><h2 id="Project-4-Pokemon"><a href="#Project-4-Pokemon" class="headerlink" title="Project 4: Pokemon"></a>Project 4: Pokemon</h2><p>This project focuses on implementing and optimizing graph algorithms through three key parts: calculating a Minimum Spanning Tree (MST) using Prim‚Äôs or Kruskal‚Äôs algorithm to minimize edge weight costs, approximating the Travelling Salesperson Problem (TSP) with heuristic-based methods for fast and nearly-optimal solutions, and solving the TSP optimally using a Branch and Bound (BnB) algorithm with efficient bounding techniques. The goal is to understand MST and BnB algorithms, evaluate their efficiency based on graph properties, and research and implement TSP heuristics like Nearest Neighbor or Christofides‚Äô algorithm. Additionally, graph visualization tools will aid in debugging and understanding graph structures, ensuring efficient, accurate, and scalable solutions for real-world computational challenges.</p><p><a href="https://github.com/kbzly/Data-Structure-Project-4.git">Project GitHub Repository</a></p>]]></content>
      
      
      <categories>
          
          <category> Projects </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structures </tag>
            
            <tag> Algorithms </tag>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YAVST</title>
      <link href="/2025/01/09/YAVST/"/>
      <url>/2025/01/09/YAVST/</url>
      
        <content type="html"><![CDATA[<h1 id="YAVST-Yet-Another-Video-Summarization-Tool"><a href="#YAVST-Yet-Another-Video-Summarization-Tool" class="headerlink" title="YAVST: Yet Another Video Summarization Tool"></a>YAVST: Yet Another Video Summarization Tool</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><strong>YAVST</strong> is a tool designed to automatically summarize long videos into short, engaging clips, suitable for platforms like TikTok. The tool leverages advanced machine learning techniques to identify the most meaningful video segments, ensuring the summarized videos maintain the essence of the original content.</p><p>By combining <strong>2D CNNs</strong> for video frame feature extraction and a <strong>transformer architecture</strong> for understanding temporal information, YAVST achieves state-of-the-art performance on the SumMe dataset, with an <strong>F1 score of 0.783</strong>.</p><h2 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h2><p>Video summarization requires understanding the identities and importance of objects within frame, as well as the dynamics of these objects across a sequence of frames. Pre-trained 2D convolutional neural network, such as ResNet, was employed to extract the features in a video frame. Extracted features was passed to a transformer network to gain temporal information. To mitigate disjoint frame summarization video was generated, kernel temporal segmentation (KTS) was employed like regularization to leverage high frame score and successive meaningful segments.</p><p><img src="/images/YAVST_workflow.png" alt="YAVST Workflow"></p><h2 id="Dataset-Description"><a href="#Dataset-Description" class="headerlink" title="Dataset Description"></a>Dataset Description</h2><p>TVSum and SumMe were chosen as dataset. TVSum contains 50 videos spanning 10 categories with lengths ranging from 2 to 10 minutes.  TVSum provides a segment-level annotation on the importance of each segment, on a scale of 1 (least important) to 5 (most important), with 20 sets of annotations for each video by different annotators. We converted the segment-level score to a frame level score by assigning the segment score to its constituent frames. SumMe also provides segment-level annotations indicating the importance of each segment within the video which designed for video summarization research.</p><p><img src="/images/YAVST_dataset.png" alt="YAVST dataset"></p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>Initially, video frames are processed through a ResNet model, a SOTA 2D CNN that can identify features within each frame efficiently. These identified features are then passed through XClip, which utilizes a SOTA cross-frame attention mechanism to analyze temporal relations and gather temporal information about events occurring across the frames, to generate a new set of features that captures the type of actions taking place in each frame. Finally, the features are weighted by a linear layer and passed to a sigmoid function to calculate the importance score for each frame.</p><p><img src="/images/YAVST_Architecture.png" alt="YAVST Architecture"></p><h2 id="Video-Summary-Generation-KTS"><a href="#Video-Summary-Generation-KTS" class="headerlink" title="Video Summary Generation: KTS"></a>Video Summary Generation: KTS</h2><p>Kernel temporal segmentation (KTS) technique was employed to ensure the continuity of the generated video. KTS method computes a set of ‚Äùchange points‚Äù that segments the input features based on the pair-wise feature similarity, so that groups of similar features, corresponding to visually contiguous segments in our case, are generated.  </p><p><img src="/images/YAVST_KTS.png" alt="YAVST KTS"></p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>The Kernel Temporal Segmentation was using to get the breaking point of the video and decision of the down-sampled frame. According to the frame rate of the video, the down-sampled frame was broadcast to all frame of the original video to make the new summarized video. The decision of the original frame of the video could be used to calculate the F1 score to evaluate the model performance both in precision and recall. Kernel Temporal Segmentation takes a long time to compute, but fortunately the F1 score is high, representing the content emphasized by the generated video and its similarity to the ratings given by human users in the ground true validation set.   </p><table><thead><tr><th>Model</th><th>F1 Score</th></tr></thead><tbody><tr><td>vsLSTM</td><td>0.376</td></tr><tr><td>dppLSTM</td><td>0.386</td></tr><tr><td>HSA-RNN</td><td>0.441</td></tr><tr><td>SUM-GDA</td><td>0.528</td></tr><tr><td>SMN</td><td>0.583</td></tr><tr><td>ResNet+KTS</td><td>0.783</td></tr><tr><td>Modified X-Clip+ResNet</td><td>0.609</td></tr></tbody></table><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p> YAVST capitalizes on the powerful image feature extraction capabilities of ResNet 101and the sophisticated temporal comprehension of the XClip transformer. Our investigation into the performance of this combination shows its potential as a robust tool for video analysis task. Despite having a slightly lower overall F1 score compared to the original simpler XClip + ResNet 101 model. The modified one‚Äôs exhibited its ability to generate discriminative importance scores between frames. To address the limitation of frame-level importance scoring, we investigated on KTS technique to seek for a better performance on video summarization. We implemented a Resnet model embedding with KTS technique. This combination allowed our model to achieve a noteworthily high F1 score of 0.783 benchmark dataset. However, since KST requires the calculation of visual similarity, the calculation method has not been optimized, resulting in a large amount of computation and time required to generate frames for the final solution of the optimization problem.</p>]]></content>
      
      
      <categories>
          
          <category> Projects </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Arm Lab</title>
      <link href="/2025/01/08/Arm%20Lab/"/>
      <url>/2025/01/08/Arm%20Lab/</url>
      
        <content type="html"><![CDATA[<h1 id="Robotic-Vision-and-Manipulation-System-for-Autonomous-Object-Handling"><a href="#Robotic-Vision-and-Manipulation-System-for-Autonomous-Object-Handling" class="headerlink" title="Robotic Vision and Manipulation System for Autonomous Object Handling"></a>Robotic Vision and Manipulation System for Autonomous Object Handling</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Given the challenge of utilizing an RX200 Arm to manipulate various types of blocks around the workspace, we applied our understanding of frame transformations, forward&#x2F;inverse kinematics, OpenCV, and programming to effectively complete these tasks. This approach allowed us to manipulate the arm, utilize camera information, detect blocks, and apply control schemes.</p><p>We document our process, methodology, and results to demonstrate our ability to apply the aforementioned concepts learned in class. Our methodology involves:</p><ol><li><strong>Camera Calibration</strong>: Calibrating the camera and implementing intrinsic, extrinsic, and homography matrices to convert between pixel, camera, and world coordinates.</li><li><strong>Kinematics</strong>: Using a DH table for forward kinematics to find the pose of the end-effector and inverse kinematics to calculate motor angles for specified poses.</li><li><strong>Block Detection</strong>: Implementing a block detector using OpenCV to create and fine-tune a blob detector for determining each block‚Äôs shape and color.</li><li><strong>Task Integration</strong>: Combining these components into a control scheme to accomplish tasks during a competition.</li></ol><p>We successfully completed most tasks in the competition, though some tasks were not scored.</p><h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><ul><li><strong>Precise Manipulation</strong>: Forward and inverse kinematics for accurate arm control.</li><li><strong>Real-Time Vision</strong>: OpenCV-based block detection, including blob detection for shapes and colors.</li><li><strong>Coordinate Transformation</strong>: Conversion between pixel, camera, and world coordinates using homography and transformation matrices.</li><li><strong>Task Automation</strong>: Integration of control schemes for competition tasks.</li></ul><h2 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h2><ol><li><p><strong>Camera Calibration</strong>:</p><ul><li>Implement intrinsic, extrinsic, and homography matrices.</li><li>Map coordinates between pixel, camera, and world frames.</li></ul></li><li><p><strong>Kinematics</strong>:</p><ul><li>Derive forward kinematics using the DH table to compute the pose of the end-effector.</li><li>Implement inverse kinematics to calculate motor angles for reaching specific end-effector poses.</li></ul></li><li><p><strong>Block Detection</strong>:</p><ul><li>Develop a blob detector using OpenCV.</li><li>Fine-tune detection algorithms to identify block shapes and colors accurately.</li></ul></li><li><p><strong>Control Scheme</strong>:</p><ul><li>Combine calibrated camera data, kinematics, and block detection into an integrated system.</li><li>Automate tasks for competition scenarios.</li></ul></li></ol><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><ul><li>Successfully completed most tasks in the competition.</li><li>Demonstrated integration of kinematics, vision, and control for robotic manipulation.</li></ul><h2 id="Repository"><a href="#Repository" class="headerlink" title="Repository"></a>Repository</h2><p>Find the project repository here: <a href="https://github.com/kbzly/Lab-Course-Part-1.git">GitHub</a></p><h2 id="Acknowledgements"><a href="#Acknowledgements" class="headerlink" title="Acknowledgements"></a>Acknowledgements</h2><p>This project was completed as part of the ARM Lab course at the University of Michigan. Special thanks to our instructors and team members for their guidance and collaboration.</p>]]></content>
      
      
      <categories>
          
          <category> Projects </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Camera </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> Robot Kinematics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ANA</title>
      <link href="/2025/01/08/ANA/"/>
      <url>/2025/01/08/ANA/</url>
      
        <content type="html"><![CDATA[<h1 id="Reproduction-and-Evaluation-of-Anytime-Nonparametric-A-Algorithm"><a href="#Reproduction-and-Evaluation-of-Anytime-Nonparametric-A-Algorithm" class="headerlink" title="Reproduction and Evaluation of Anytime Nonparametric A* Algorithm"></a>Reproduction and Evaluation of Anytime Nonparametric A* Algorithm</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li><p><strong>Reproduction</strong>: Reproduced the Anytime Nonparametric A* (ANA*) algorithm.</p></li><li><p><strong>Application</strong>: Designed specific robotic application scenarios to test the algorithm.</p></li><li><p><strong>Evaluation</strong>: Conducted a performance comparison with other path planning algorithms.</p></li><li><p><strong>Repository</strong>: <a href="https://github.com/kbzly/Anytime-Nonparametric-A-Algorithm.git">GitHub</a></p></li></ul><h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><ul><li><strong>Efficient Pathfinding</strong>: Uses nonparametric techniques to optimize performance and computational cost.</li><li><strong>Anytime Behavior</strong>: Quickly finds a feasible solution and refines it over time.</li><li><strong>Adaptability</strong>: Performs well in diverse robotic application scenarios.</li><li><strong>Comparison-Ready</strong>: Includes benchmarks against traditional path planning algorithms.</li></ul><h2 id="Pseudocode"><a href="#Pseudocode" class="headerlink" title="Pseudocode"></a>Pseudocode</h2><p>Here is an overview of the pseudocode for the <strong>Anytime Nonparametric A</strong>* algorithm:</p><p><img src="/images/ANA_pseudocode.png" alt="Pseudocode"></p><p><em>Figure: Pseudocode for the Anytime Nonparametric A</em> Algorithm.*</p><h2 id="Customized-Environment"><a href="#Customized-Environment" class="headerlink" title="Customized Environment"></a>Customized Environment</h2><p>Below is a sample customized environment used to evaluate the algorithm:</p><p><img src="/images/customized_environment.png" alt="Customized Environment"></p><p><em>Figure: Customized environment created for testing Anytime Nonparametric A</em> Algorithm.*</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Here is a sample result generated by the <strong>Anytime Nonparametric A*</strong> algorithm:</p><p><img src="/images/Result1.png" alt="Result"></p><p><em>Figure: Path planning result visualized for the customized environment.</em></p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>The Anytime Nonparametric A* (ANA*) algorithm offers the following insights:</p><ul><li>ANA* can quickly find a suboptimal solution and efficiently refine it to the optimal solution.</li><li>Its primary strength lies in adaptively maintaining the greediest search for fast suboptimal solutions rather than relying on a smart heuristic.</li><li>It is better suited to small but complex environments where its advantages can be fully utilized.</li><li>Using a <strong>Weighted Euclidean heuristic</strong> instead of a standard Euclidean heuristic can result in smoother paths in 8-connected spaces, making it more realistic for practical applications.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Projects </category>
          
      </categories>
      
      
        <tags>
            
            <tag> robotics </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MBot</title>
      <link href="/2025/01/08/Mbot/"/>
      <url>/2025/01/08/Mbot/</url>
      
        <content type="html"><![CDATA[<h1 id="MBot-Autonomous-Ground-Robot"><a href="#MBot-Autonomous-Ground-Robot" class="headerlink" title="MBot Autonomous Ground Robot"></a>MBot Autonomous Ground Robot</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Our project demonstrates key capabilities required by modern autonomous systems using the <strong>MBot</strong>, a three-level differential drive robot equipped with:</p><ul><li>DC motors and encoders for precise motion control.</li><li>A forward-facing camera for block detection using April Tags.</li><li>A LIDAR scanner for navigation and mapping.</li><li>A fabricated forklift mechanism for object interaction.</li></ul><p>The system integrates motion control, SLAM, path planning, and object manipulation, enabling the MBot to complete tasks like efficient exploration and object handling.</p><hr><h2 id="Motion-Controller-Odometry"><a href="#Motion-Controller-Odometry" class="headerlink" title="Motion Controller &amp; Odometry"></a>Motion Controller &amp; Odometry</h2><h3 id="Calibration"><a href="#Calibration" class="headerlink" title="Calibration"></a>Calibration</h3><p>MBot uses the Classic Calibrate script to calibrate the Mbot automatically and ensure that the polarity of the robot‚Äôs encoders match the software implementation. The calibration data is stored in the onboard Pico‚Äôs memory.  </p><h3 id="Odometry"><a href="#Odometry" class="headerlink" title="Odometry"></a>Odometry</h3><p>When comparing our actual Mbot rotation angle œâ to a predetermined desired angle, we found a large error. To compensate, an updated method to set our angular velocity was implemented, which directly uses the gyro data to calculate rotation angle. We used Mbot dynamic model to calculate left and right wheel speed to update our odometry.   </p><h3 id="PID-Controller-and-Filters"><a href="#PID-Controller-and-Filters" class="headerlink" title="PID Controller and Filters"></a>PID Controller and Filters</h3><p>It is possible to drive the Mbot without a PID motor speed controller. However, applying an unfiltered pwm signal results in very jerky movements and the Mbot falls over more often than not. Implementing a proportional, integral, derivative controller allows for smooth and accurate speed adjustments. Our PID controller uses our calibration values as well as the wheel encoders to provide feedback on the wheel‚Äôs speed. The motor speed controller is implemented in the MBot loop function when the drive mode is ‚ÄúMODE MBOT VEL.‚Äù The measured speed of the motors, read using the motor‚Äôs encoders, is passed through a low pass filter to reduce the noise from discretization. The difference between the desired and measured velocity is used as an input parameter for the rc filter march which outputs the right and left motor command. The velocity command is then converted to a pwm signal.</p><h3 id="Control-Algorithm"><a href="#Control-Algorithm" class="headerlink" title="Control Algorithm"></a>Control Algorithm</h3><ul><li><strong>Straight Maneuver Controller:</strong> Adjusts velocity to maintain heading toward the goal.</li><li><strong>Turn Maneuver Controller:</strong> Controls angular velocity for precise rotations.</li><li><strong>Smart Maneuver Controller:</strong> Combines straight movement and angular adjustments.</li></ul><hr><h2 id="Simultaneous-Localization-and-Mapping-SLAM"><a href="#Simultaneous-Localization-and-Mapping-SLAM" class="headerlink" title="Simultaneous Localization and Mapping (SLAM)"></a>Simultaneous Localization and Mapping (SLAM)</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>SLAM combines data from LIDAR, odometry, and a particle filter to map and localize MBot. This is essential in environments without GPS or pre-existing maps.</p><h3 id="Mapping"><a href="#Mapping" class="headerlink" title="Mapping"></a>Mapping</h3><p>A 2D occupancy grid updates based on LIDAR scans:</p><ul><li>Endpoints of rays represent obstacles.</li><li>Points between the start and endpoint are marked as free using Bresenham‚Äôs Algorithm.</li></ul><h3 id="Action-Model"><a href="#Action-Model" class="headerlink" title="Action Model"></a>Action Model</h3><p>The MBot predicts its next pose by modeling its movements as a combination of:</p><ol><li>Rotation.</li><li>Translation.</li><li>Another rotation.</li></ol><p>Gaussian noise is added to account for uncertainties, balancing accuracy and efficiency.</p><h3 id="Sensor-Model-and-Particle-Filter"><a href="#Sensor-Model-and-Particle-Filter" class="headerlink" title="Sensor Model and Particle Filter"></a>Sensor Model and Particle Filter</h3><p>The particle filter evaluates potential poses based on LIDAR data. The most likely pose is determined by weighted particles, ensuring accurate localization.</p><hr><h2 id="Path-Planning"><a href="#Path-Planning" class="headerlink" title="Path Planning"></a>Path Planning</h2><h3 id="A-Path-Planning"><a href="#A-Path-Planning" class="headerlink" title="A* Path Planning"></a>A* Path Planning</h3><p>The A* algorithm finds the shortest path between two points using:</p><ul><li><strong>g-cost:</strong> Distance from the start to the current node.</li><li><strong>h-cost:</strong> Estimated distance to the goal.</li><li><strong>f-cost:</strong> Sum of g-cost and h-cost.</li></ul><p>Nodes are evaluated in 8 directions (up, down, left, right, and diagonals) for optimal path selection.</p><h3 id="Map-Exploration"><a href="#Map-Exploration" class="headerlink" title="Map Exploration"></a>Map Exploration</h3><p>The MBot explores using frontiers, which are free, unexplored areas identified by LIDAR. It:</p><ol><li>Plans a path to the nearest frontier.</li><li>Explores until no reachable frontiers remain.</li><li>Returns to the starting position.</li></ol><hr><h2 id="Forklift-Operations"><a href="#Forklift-Operations" class="headerlink" title="Forklift Operations"></a>Forklift Operations</h2><h3 id="Design-and-Fabrication"><a href="#Design-and-Fabrication" class="headerlink" title="Design and Fabrication"></a>Design and Fabrication</h3><p>The forklift uses a <strong>Scotch yoke mechanism</strong> for vertical movement, meeting three constraints:</p><ol><li>Aligns with pallets for pickup and clears a height &gt;120 mm for drop-off.</li><li>Minimizes obstruction of the camera and LIDAR.</li><li>Operates efficiently with a limited motor range.</li></ol><h3 id="Interacting-with-Crates"><a href="#Interacting-with-Crates" class="headerlink" title="Interacting with Crates"></a>Interacting with Crates</h3><ul><li><strong>Pickup Protocol:</strong> Identifies and lifts blocks using April Tags to determine ID, location, and height.</li><li><strong>Drop-Off Protocol:</strong> Places blocks in specified locations, ensuring proper stacking.</li></ul><p>The forklift operates at four preset heights to handle crates efficiently, based on experimentally determined positions.</p>]]></content>
      
      
      <categories>
          
          <category> Projects </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Autonomous Robots </tag>
            
            <tag> Path Planning </tag>
            
            <tag> SLAM </tag>
            
            <tag> LIDAR </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
